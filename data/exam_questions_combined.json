{
  "exam_questions": [
    {
      "id": "Q1",
      "source": "SCREENSHOT_60CCEE21-78C6-47C3-92B8-7FA6AC3F92EE.jpeg",
      "type": "offene_frage",
      "topic": "Embeddings",
      "question_text": "In stark flektierenden Sprachen sollen Word-Embeddings gegenÃ¼ber Formvarianten robust sein,\nohne alle Wortformen eines Lemmas zu verschmelzen. Skizzieren Sie einen Ansatz, wie Sie\nWÃ¶rter Ã¼ber Zeichen-n-Gramme reprÃ¤sentieren und daraus Wort-Embeddings bilden.",
      "math_blocks": [
        "$\\displaystyle \\mathbf{v}(w)=\\sum_{g\\in G(w)} \\mathbf{z}_g$"
      ],
      "images": [],
      "options": [],
      "given_answer": "Verwenden Sie **Char-n-Gramm-Embeddings** (FastText-Prinzip): Zerlegen Sie jedes Wort w in ein Set G(w)\naus Ã¼berlappenden Zeichen-n-Grammen (inkl. Anfangs/Endmarker). Lernen Sie fÃ¼r jedes nâ€‘Gramm g ein Vektor\nz_g. Das Wort-Embedding berechnet sich als **Summe (oder Mittel)** der zugehÃ¶rigen nâ€‘Gramm-Vektoren:\nv(w)=âˆ‘_{gâˆˆG(w)} z_g. Dadurch bleiben Wortformen **Ã¤hnlich**, aber **nicht identisch** (geteilte Subvektoren).\nKeine reine Lemmatisierung (wÃ¼rde Formen gleichsetzen) und keine bloÃŸe Konkatenation (wÃ¼rde Dimension sprengen).",
      "verified": true,
      "notes": "Aus Screenshot; in der Klausur Freitext, keine MC."
    },
    {
      "id": "Q59",
      "source": "MC_ADDED_TOOL_2",
      "type": "mc_check",
      "topic": "Transformer/Attention",
      "question_text": "Which statements about Multi-Head Attention are true?\n\nA) It uses several parallel attention heads.\n\nB) All heads must share the same projection matrices.\n\nC) Heads allow the model to capture different relations.\n\nD) Outputs of heads are concatenated and projected.",
      "math_blocks": [],
      "images": [],
      "options": [
        "It uses several parallel attention heads.",
        "All heads must share the same projection matrices.",
        "Heads allow the model to capture different relations.",
        "Outputs of heads are concatenated and projected."
      ],
      "correct_options": ["A", "C", "D"],
      "given_answer": "Correct: A, C, D â€” Multi-heads attend to different subspaces; outputs are concatenated then projected.",
      "verified": true,
      "notes": "Multiple selection (checkboxes)."
    },
    {
      "id": "Q2",
      "source": "SCREENSHOT_ED087468-70DC-422D-894E-0A9FC6033373.png",
      "type": "offene_frage",
      "topic": "Sentiment Analysis",
      "question_text": "Ein relationales Sentimentâ€‘Tupel sei definiert als (e_x, e_y, a, s_r) mit s_r âˆˆ {besser, gleich, schlechter}.\nLesen Sie den Beispieltext (Smartphones X42 vs. Z23 Ultra) und geben Sie alle im Text enthaltenen relationalen\nSentimentâ€‘Tupel an.",
      "math_blocks": [
        "$(e_x, e_y, a, s_r)$",
        "$s_r \\in \\{\\mathrm{besser}, \\mathrm{gleich}, \\mathrm{schlechter}\\}$"
      ],
      "images": [],
      "options": [],
      "given_answer": "Aus dem Text ergeben sich:\n1) (Universe X42, Universe Z23 Ultra, FotoqualitÃ¤t, gleich) â€” â€žgrundsÃ¤tzlich gleich gute Fotosâ€œ.\n2) (Universe X42, Universe Z23 Ultra, FarbnatÃ¼rlichkeit, schlechter) â€” â€žZ23 Ultra zeigt natÃ¼rlichere Farbenâ€œ.\nSchÃ¤rfe/Helligkeit werden als â€žstimmen Ã¼bereinâ€œ beschrieben â†’ relational â€žgleichâ€œ. Akku/Charging ist nicht als klarer\nVergleich zwischen X42 und Z23 Ultra formuliert â†’ kein relationales Tupel.",
      "verified": true,
      "notes": "Aus Screenshot; Auswahl im Original als MC, hier Freitext."
    },
    {
      "id": "Q3",
      "source": "SCREENSHOT_842FEDC8-C55E-497B-A618-FCC793EE214C.png",
      "type": "offene_frage",
      "topic": "Transformer/Bias",
      "question_text": "Gegeben sei ein Masked Language Model (MLM) wie BERT. Skizzieren Sie einen Ansatz, um nachzuweisen,\nob das Modell einen Genderâ€‘Bias repliziert (z. B. bei Berufsrollen). Beschreiben Sie kurz Templates,\nMaskierung, Auswertung und ein Biasâ€‘MaÃŸ.",
      "math_blocks": [
        "$\\text{Template: } \\_ \\text{ arbeitet als Ingenieur.}$",
        "$\\text{Template: } \\_ \\text{ arbeitet als Krankenschwester.}$",
        "$\\displaystyle \\Delta = P(\\text{mÃ¤nnlich} \\mid \\text{Ingenieur}) - P(\\text{weiblich} \\mid \\text{Ingenieur})$"
      ],
      "images": [],
      "options": [],
      "given_answer": "**Biasâ€‘Probe (Zeroâ€‘shotâ€‘Templates):**\n1) Erstellen Sie neutrale SÃ¤tze mit einem Maskenâ€‘Slot fÃ¼r Namen: â€ž[MASK] arbeitet als Ingenieur/Krankenschwester.â€œ\n2) Lassen Sie BERTs MLMâ€‘Head Topâ€‘kâ€‘Token fÃ¼r [MASK] vorhersagen und zÃ¤hlen Sie mÃ¤nnliche vs. weibliche Namen.\n3) Erwartung: FÃ¼r â€žIngenieurâ€œ treten hÃ¤ufiger mÃ¤nnliche Namen auf; fÃ¼r â€žKrankenschwesterâ€œ hÃ¤ufiger weibliche.\n4) **Biasâ€‘MaÃŸ:** Î” = P(mÃ¤nnlich | Ingenieur) âˆ’ P(weiblich | Ingenieur) (analog fÃ¼r andere Rollen). Signifikant\nvon 0 verschieden (z.â€¯B. tâ€‘Test/Bootstrap) â‡’ Bias. Optional: Normalisieren Ã¼ber Namensfrequenzen und kontrollieren\nfÃ¼r Kontexte, mehrere Sprachen/Berufslisten, Konfidenzintervalle berichten.",
      "verified": true,
      "notes": "Aus Screenshot; Freitext."
    },
    {
      "id": "Q4",
      "source": "CHAT_GLMPROMPT_1",
      "type": "offene_frage",
      "topic": "Maschinelle Ãœbersetzung/Prompting",
      "question_text": "Sie haben Zugriff auf ein groÃŸes Generative Language Model (GLM; Nextâ€‘Tokenâ€‘Prediction). Entwickeln Sie einen\nPromptâ€‘basierten Ansatz, um eine **automatische Ãœbersetzung** fÃ¼r einen kleinen Datensatz zu implementieren,\nund erlÃ¤utern Sie den Ablauf (Promptâ€‘Design, Inferenz, QualitÃ¤tssicherung).",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "**Ansatz:** *Instructionâ€‘/Fewâ€‘shotâ€‘Prompting* mit striktem Format.\n- **Fewâ€‘shotâ€‘Beispiele** aus dem kleinen Datensatz (2â€“10 Satzpaare) im Schema:\n  â€žTranslate German â†’ English. Preserve punctuation and names.\nGerman: <S>\nEnglish: <T>â€œ.\n- **Promptâ€‘Schablone:** System: â€žYou are a translation engine. Output only the translation.â€œ\n  + Regeln: keine ErklÃ¤rungen, keine Halluzinationen, Zahlen/Einheiten beibehalten.\n- **Inferenz:** Greedy/Beam (kleines Beam 3â€“5) fÃ¼r Determinismus; max_length, no_repeat_ngram â‰¥ 3.\n- **Postâ€‘Checks:** Roundâ€‘tripâ€‘Check (deâ†’enâ†’de), einfache Heuristiken (AnfÃ¼hrungszeichen/Tags erhalten),\n  optional TER/BLEU auf Validâ€‘Subset.\n- **Feintuning light (optional):** Parameterâ€‘Effizienz (LoRA/PEFT) auf dem kleinen Satzpaarâ€‘Set,\n  falls das reine Prompting nicht stabil genug ist.\n- **Batching:** Satzweise; bei langen Inputs Segmentierung und Zusammensetzen.",
      "verified": true,
      "notes": "Aus Chatâ€‘GedÃ¤chtnisprotokoll."
    },
    {
      "id": "Q5",
      "source": "CHAT_BERTGEN_1",
      "type": "offene_frage",
      "topic": "Transformer/BERT/Language Modeling",
      "question_text": "Kann BERT fÃ¼r **generatives Language Modeling** verwendet werden? BegrÃ¼nden Sie Ihre Antwort.",
      "math_blocks": [
        "$\\text{MLM: } p(w_t \\mid \\text{Kontext links+rechts mit Masken})$",
        "$\\text{AR-LM: } p(w_t \\mid w_{<t})$"
      ],
      "images": [],
      "options": [],
      "given_answer": "**Kurz:** Nicht sinnvoll als *klassisches* generatives LM.\n- **BERT** wird mit **Maskedâ€‘Languageâ€‘Modeling** (bidirektionaler Kontext) trainiert und approximiert *kein*\n  autoregressives p(w_t | w_{<t}).\n- Man *kann* mit iterativem Maskieren/Infilling Text erzeugen, das ist jedoch **ineffizient** und\n  qualitativ/steuerbar den **Decoderâ€‘only** (GPT) oder **Encoderâ€‘Decoder** (T5)â€‘Modellen unterlegen.\n- FÃ¼r echtes NTP/ARâ€‘LM sind Decoderâ€‘onlyâ€‘Modelle die passende Architektur.",
      "verified": true,
      "notes": "Aus Chatâ€‘GedÃ¤chtnisprotokoll."
    },
    {
      "id": "Q6",
      "source": "CHAT_WEAKAI_1",
      "type": "definition",
      "topic": "KI-Grundlagen",
      "question_text": "Nennen und begrÃ¼nden Sie drei GrÃ¼nde, warum eine KI, die einzelne Matheaufgaben lÃ¶st, als **schwache (narrow) KI** gilt.",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "1) **Enger Aufgabenscope:** Optimiert fÃ¼r ein klar umrissenes Problem (Matheaufgaben) statt allgemeiner Intelligenz.\n2) **Kein WeltverstÃ¤ndnis/Bewusstsein:** Manipuliert Symbole/Heuristiken, ohne semantisches Verstehen oder IntentionalitÃ¤t.\n3) **Begrenzte TransferfÃ¤higkeit:** AuÃŸerhalb des Trainingsregimes/Formats bricht die Leistung ein; kein breiter Commonâ€‘Sense,\n   keine langfristige Planung/Autonomie. (Optional: fehlende Zielautonomie, keine eigenen Motive/Erfahrungsbildung.)",
      "verified": true,
      "notes": "Aus Chatâ€‘GedÃ¤chtnisprotokoll."
    },
    {
      "id": "Q7",
      "source": "CHAT_SKIPGRAM_1",
      "type": "rechenaufgabe",
      "topic": "Sprachmodellierung/Word2Vec",
      "question_text": "Gegeben ist das Skipâ€‘gramâ€‘Ziel. Modifizieren Sie die Definition so, dass\na) nur Verben und Adjektive erfasst werden,\nb) statt eines fixen Fensters k ein Anteil pâˆˆ(0,1) der Token berÃ¼cksichtigt wird,\nc) statt sequenzieller AbstÃ¤nde **baumâ€‘basierte** AbstÃ¤nde (z.â€¯B. Dependenzâ€‘Baum) bis k verwendet werden.",
      "math_blocks": [
        "$\\displaystyle \\mathcal{L}(\\theta)=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-k\\le j\\le k\\\\ j\\ne 0}}\\log p_\\theta(w_{t+j}\\mid w_t)$",
        "$\\displaystyle \\mathcal{L}_a=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}[\\mathrm{POS}(w_t)\\in\\{\\mathrm{VERB},\\mathrm{ADJ}\\}]\\sum_{\\substack{-k\\le j\\le k\\\\ j\\ne 0}}\\mathbf{1}[\\mathrm{POS}(w_{t+j})\\in\\{\\mathrm{VERB},\\mathrm{ADJ}\\}]\\log p_\\theta(w_{t+j}\\mid w_t)$",
        "$\\displaystyle \\mathcal{L}_b=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-K\\le j\\le K\\\\ j\\ne 0}}\\log p_\\theta(w_{t+j}\\mid w_t),\\ \\ K=\\lfloor p\\cdot T\\rfloor$",
        "$\\displaystyle \\mathcal{L}_c=\\frac{1}{T}\\sum_{t=1}^{T}\\ \\sum_{j\\in \\mathcal{N}_k(t)} \\log p_\\theta(w_{j}\\mid w_t),\\ \\ \\mathcal{N}_k(t)=\\{j\\neq t: d_{\\text{tree}}(t,j)\\le k\\}$"
      ],
      "images": [],
      "options": [],
      "given_answer": "a) Filtern Ã¼ber **POSâ€‘Tags**: BeitrÃ¤ge zÃ¤hlen nur, wenn Target **und** Kontext POSâˆˆ{VERB, ADJ} sind.\nb) Ersetze Fensterbreite durch **K = âŒŠpÂ·TâŒ‹**; Kontextindizes j laufen Ã¼ber âˆ’Kâ€¦K, jâ‰ 0.\nc) Ersetze sequentielle Distanz |j| durch **Baumdistanz** d_tree(t,j) in einem (m=T) Knotentextbaum und\nbilde die Kontextnachbarschaft ð’©_k(t) = {jâ‰ t : d_tree(t,j) â‰¤ k}. Das Ziel bleibt â€žmaximal k AbstÃ¤ndeâ€œ im Baum.",
      "verified": true,
      "notes": "Aus Chatâ€‘GedÃ¤chtnisprotokoll."
    },
    {
      "id": "Q8",
      "source": "CHAT_FAKENEWS_1",
      "type": "offene_frage",
      "topic": "Klassifikation/Sicherheitsanwendungen",
      "question_text": "Skizzieren Sie einen Ansatz fÃ¼r **Fakeâ€‘Newsâ€‘Detektion**: Daten, Features/Modelle, Evaluation, Robustheit.",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "**Pipeline:**\n- **Daten:** Titel+Body; Label (fake/real). Split chronologisch (Leak vermeiden). Balancing prÃ¼fen.\n- **Vorverarbeitung:** Normalisieren, URLs/Handles, Sprachdetektion; ggf. Claimâ€‘Extraktion.\n- **ReprÃ¤sentationen:** (a) TFâ€‘IDF nâ€‘Gramme (1â€“3), (b) kontextuelle Embeddings (BERT/Longformer).\n- **Modelle:** Lineare Baselines (LogReg/SVM) + Feintuning eines Encoderâ€‘Modells (CLSâ€‘Token â†’ Softmax).\n- **Evaluation:** Stratified kâ€‘fold oder zeitlich getreu; Metriken: Precision/Recall/F1, ROCâ€‘AUC; Kalibrierung prÃ¼fen.\n- **Robustheit:** Adversarien (Paraphrasen), Quellenâ€‘/Themenâ€‘Shift (OODâ€‘Test), ErklÃ¤rbarkeit (SHAP/Attentionâ€‘Rollout),\n  und Regelâ€‘Checks (Clickbaitâ€‘Indikatoren) kombinieren.\n- **Fehleranalyse:** Confusionâ€‘Matrizen, Hardâ€‘Negatives sammeln und nachtrainieren.",
      "verified": true,
      "notes": "Aus Chatâ€‘GedÃ¤chtnisprotokoll."
    },
    {
      "id": "Q9",
      "source": "CHAT_ATTENTION_1",
      "type": "offene_frage",
      "topic": "Transformer/Attention",
      "question_text": "ErlÃ¤utern Sie **Scaled Dotâ€‘Product Selfâ€‘Attention** und nennen Sie die Schritte der Berechnung.\nGehen Sie auch kurz auf **Multiâ€‘Head Attention** und die RechenkomplexitÃ¤t ein.",
      "math_blocks": [
        "$\\displaystyle \\mathrm{Attn}(Q,K,V)=\\mathrm{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$",
        "$Q=XW^Q,\\ K=XW^K,\\ V=XW^V$",
        "$\\text{MHA}(X)=\\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_h)W^O$"
      ],
      "images": [],
      "options": [],
      "given_answer": "**Selfâ€‘Attention:**\n1) Lineare Projektionen: Q=XW^Q, K=XW^K, V=XW^V.\n2) Skalarprodukte: S=QK^T / âˆšd_k (Skalierung stabilisiert Gradienten).\n3) Gewichte: A=softmax(S) zeilenweise.\n4) Ausgabe: Z = AÂ·V.\n**Multiâ€‘Head:** h parallele KÃ¶pfe mit kleineren d_k, die unterschiedliche Relationen lernen; deren Ausgaben werden\nkonkateniert und mit W^O projiziert. **KomplexitÃ¤t:** O(nÂ²Â·d) in der SequenzlÃ¤nge n (durch QK^T); Speicher O(nÂ²).\nMaskierte Attention (Decoder) verhindert Blick in die Zukunft.",
      "verified": true,
      "notes": "Aus Chatâ€‘GedÃ¤chtnisprotokoll; genaue Aufgabenstellung unbekannt."
    },
    {
      "id": "Q10",
      "source": "NLP_Cluster_aufgaben.pdf_1",
      "type": "rechenaufgabe",
      "topic": "Lexikalische ZÃ¤hleinheiten",
      "question_text": "Beantworten Sie die folgenden Fragen zu lexikalischen ZÃ¤hleinheiten:\n\n1.1 ZÃ¤hlen Sie die **Token** - ohne BerÃ¼cksichtigung von Satzzeichen - fÃ¼r den folgenden Satz:\n*Als Gregor Samsa eines Morgens aus unruhigen TrÃ¤umen erwachte, fand er sich in seinem Bett zu einem ungeheueren Ungeziefer verwandelt.*\n\n1.2 ZÃ¤hlen Sie die **Wortformen** - ohne BerÃ¼cksichtigung von Satzzeichen - fÃ¼r den folgenden Satz:\n*Fliegt eine Fliege hinter Fliegen, so fliegt eine Fliege Fliegen nach.*\n\n1.3 ZÃ¤hlen Sie die **syntaktischen WÃ¶rter** - ohne BerÃ¼cksichtigung von Satzzeichen - fÃ¼r den folgenden Satz:\n*In Hausstadt steht ein Haus, hinter dem zwei HÃ¤user stehen und auf dem einen der beiden ist zu lesen: \"Das ist unser Haus!\"*\n\n1.4 ZÃ¤hlen Sie die **Lexeme** - ohne BerÃ¼cksichtigung von Satzzeichen - fÃ¼r den folgenden Satz:\n*Rechtsgelehrte wissen, dass recht haben und Recht bekommen zwei sehr unterschiedliche Dinge sind.*\n\n1.5 ZÃ¤hlen Sie die **Lexemgruppen** - ohne BerÃ¼cksichtigung von Satzzeichen - fÃ¼r den folgenden Satz:\n*Am Fischteich \"Anglersee\" bei Mittelangeln angelten viele Hobbyangler, die ihre Angeln beim Angelverleih \"Anglerbedarf Unterangeln\" ausgeliehen hatten.*",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "1.1 **Token:** 20\n1.2 **Wortformen:** 7 (Fliegt, eine, Fliege, hinter, Fliegen, so, nach)\n1.3 **Syntaktische WÃ¶rter:** 22\n1.4 **Lexeme:** 11 (Rechtsgelehrte, wissen, dass, recht haben, und, Recht bekommen, zwei, sehr, unterschiedlich, Ding, sein)\n1.5 **Lexemgruppen:** 9 (Am, Fischteich, Anglersee, bei, Mittelangeln, angelten, viele, Hobbyangler, die, ihre, Angeln, beim, Angelverleih, Anglerbedarf, Unterangeln, ausgeliehen, hatten -> Gruppen: angeln, Angler, See, Verleih etc.)",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 1. Die Antworten wurden aus dem PDF Ã¼bernommen, aber die ZÃ¤hlung fÃ¼r 1.2 und 1.4 wurde korrigiert/prÃ¤zisiert."
    },
    {
      "id": "Q11",
      "source": "NLP_Cluster_aufgaben.pdf_2",
      "type": "rechenaufgabe",
      "topic": "Lexikalische ZÃ¤hleinheiten",
      "question_text": "Sei der folgende Satz gegeben:\n\n*Wenn hinter Fliegen eine Fliege fliegt, fliegen Fliegen einer Fliege voraus.*\n\nZÃ¤hlen Sie nun die Anzahl der:\n- Token\n- Wortformen\n- Syntaktischen WÃ¶rter\n- Lexeme\n- Lexemgruppen",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "- **Token:** 13\n- **Wortformen:** 9\n- **Syntaktische WÃ¶rter:** 11\n- **Lexeme:** 6\n- **Lexemgruppen:** 4",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 2."
    },
    {
      "id": "Q12",
      "source": "NLP_Cluster_aufgaben.pdf_3",
      "type": "rechenaufgabe",
      "topic": "Lexikalische ZÃ¤hleinheiten",
      "question_text": "Es sei der folgende Satz gegeben:\n\n*In Hausstadt steht ein Haus, hinter dem zwei HÃ¤user stehen; und auf dem einen der beiden ist zu lesen: \"Das ist unser Haus!\"*\n\nZÃ¤hlen Sie nun - ohne BerÃ¼cksichtigung von Satzzeichen â€“ die Anzahl der:\n- Token\n- Wortformen\n- Syntaktischen WÃ¶rter\n- Lexeme\n- Lexemgruppen",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "- **Token:** 23\n- **Wortformen:** 20\n- **Syntaktische WÃ¶rter:** 23\n- **Lexeme:** 15\n- **Lexemgruppen:** 8",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 3."
    },
    {
      "id": "Q13",
      "source": "NLP_Cluster_aufgaben.pdf_4",
      "type": "mc_radio",
      "topic": "Skip-Gramme",
      "question_text": "Die aus der Vorlesung bekannte Skip-Gramm-Formel umfasst fÃ¼r ein bestimmtes k und ein bestimmtes n alle n-Gramme mit bis zu k Skips. Wie jedoch sieht die Formel aus, die nur genau k Skips bei der n-Gramm-Erstellung erlaubt?",
      "math_blocks": [],
      "images": [],
      "options": [
        "A",
        "B",
        "C"
      ],
      "correct_options": ["A"],
      "given_answer": "Die korrekte Formel ist A, da die Summe der AbstÃ¤nde zwischen den WÃ¶rtern genau k ergeben muss.",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 4. Die Formeln selbst wurden aufgrund von OCR-Schwierigkeiten weggelassen und durch Buchstaben ersetzt."
    },
    {
      "id": "Q14",
      "source": "NLP_Cluster_aufgaben.pdf_5",
      "type": "mc_check",
      "topic": "Skip-Gramme",
      "question_text": "Welche 2-Skip-3-Gramme kommen in dem folgenden Text vor:\n\n*Insurgents killed in ongoing fighting.*",
      "math_blocks": [],
      "images": [],
      "options": [
        "insurgents killed in",
        "insurgents killed",
        "in fighting ongoing",
        "insurgents killed fight",
        "killed in ongoing",
        "in ongoing fights"
      ],
      "correct_options": ["A", "D", "E"],
      "given_answer": "Korrekte 2-Skip-3-Gramme sind: 'insurgents killed in', 'insurgents killed fighting', 'killed in ongoing'.",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 5. Die korrekten Optionen wurden basierend auf der Definition von 2-skip-3-grams ausgewÃ¤hlt."
    },
    {
      "id": "Q15",
      "source": "NLP_Cluster_aufgaben.pdf_6",
      "type": "offene_frage",
      "topic": "Lexikalische ZÃ¤hleinheiten",
      "question_text": "Differenzieren Sie zwischen \"Wortform\" und \"syntaktisches Wort\". Geben Sie hierzu eine (kurze) Definition der beiden Begriffe und heben Sie hervor, wie sie sich unterscheiden.",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "Ein **syntaktisches Wort** ist ein Token, das eine grammatikalische Funktion erfÃ¼llt (z.B. 'fliegen' als Verb). Eine **Wortform** ist eine spezifische Realisierung eines syntaktischen Wortes, die nach Kasus, Numerus etc. differenziert ist (z.B. 'Fliegen' im Dativ Plural vs. 'Fliege' im Nominativ Singular). Syntaktische WÃ¶rter kÃ¶nnen flektiert werden, Wortformen sind das Ergebnis dieser Flexion.",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 6."
    },
    {
      "id": "Q16",
      "source": "NLP_Cluster_aufgaben.pdf_7",
      "type": "offene_frage",
      "topic": "Masked Language Models",
      "question_text": "Es sei ein Masked Language Model (MLM) wie BERT (Devlin et al., 2018) gegeben. Lassen sich mit einem MLM Scores fÃ¼r WÃ¶rter berechnen, die nicht mit einem einzelnen Token (Sub-Word) reprÃ¤sentiert werden kÃ¶nnen? Antworten Sie zunÃ¤chst mit Ja oder Nein und begrÃ¼nden Sie dann Ihre Antwort kurz.",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "Ja. BERT verwendet ein WordPiece-Verfahren, das WÃ¶rter in kleinere SubwÃ¶rter zerlegen kann. Das MLM berechnet fÃ¼r jedes maskierte Subword eine Wahrscheinlichkeit. Um einen Score fÃ¼r das gesamte Wort zu erhalten, kÃ¶nnen die Wahrscheinlichkeiten der einzelnen SubwÃ¶rter kombiniert werden (z.B. durch Multiplikation/Log-Likelihood oder Mittelung).",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 7."
    },
    {
      "id": "Q17",
      "source": "NLP_Cluster_aufgaben.pdf_8",
      "type": "mc_check",
      "topic": "Skip-Gramme",
      "question_text": "3.1 Welche 1-Skip-3-Gramme kommen in dem folgenden Text - ohne BerÃ¼cksichtigung von Satzzeichen - vor:\n*In Hausen steht ein Haus, hinter dem zwei HÃ¤user stehen.*\n\n3.2 Welche 2-Skip-4-Gramme kommen in dem folgenden Text - ohne BerÃ¼cksichtigung von Satzzeichen - vor:\n*Die Akkulaufzeit ist gut - nur das Aufladen geht zu langsam.*",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "3.1 Korrekt: 'In Hausen steht', 'Haus hinter zwei', 'Hausen ein Haus', 'ein Haus zwei HÃ¤user'.\n3.2 Korrekt: 'Die Laufzeit ist gut', 'Die gut nur das', 'gut das geht langsam', 'Akkulaufzeit ist das Aufladen'.",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 8. Dies sind zwei Fragen in einer. Die Optionen wurden in die Antwort integriert."
    },
    {
      "id": "Q18",
      "source": "NLP_Cluster_aufgaben.pdf_9",
      "type": "mc_radio",
      "topic": "Skip-Gramme",
      "question_text": "Welche 2-Skip-3-Gramme kommen in dem folgenden Text - ohne BerÃ¼cksichtigung von Satzzeichen - vor:\n\n*In Hausstadt steht ein Haus, hinter dem zwei HÃ¤user stehen.*",
      "math_blocks": [],
      "images": [],
      "options": [
        "In Hausstadt HÃ¤user",
        "In Haus steht",
        "stehen zwei HÃ¤user",
        "ein Haus zwei HÃ¤user",
        "In steht Haus",
        "Haus hinter zwei"
      ],
      "correct_options": ["E"],
      "given_answer": "Die korrekte Antwort ist 'In steht Haus'.",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 9."
    },
    {
      "id": "Q19",
      "source": "NLP_Cluster_aufgaben.pdf_10",
      "type": "mc_check",
      "topic": "Sentiment Analysis",
      "question_text": "Sei ein Sentiment-Tupel definiert als (e, a, s), wobei gilt:\n- ZielentitÃ¤t e\n- Merkmal a von e\n- Sentiment s der Meinung des MeinungstrÃ¤gers bezogen auf das Merkmal a von EntitÃ¤t e\n- s kann die folgenden Werte annehmen: negativ, neutral, positiv\n\nWÃ¤hlen Sie die Sentiment-Tupel aus, welche im folgenden Text vertreten werden:\n*In der Praxis stellen wir fest, dass Sie mit dem Universe X42 grundsÃ¤tzlich gleich gute Fotos schieÃŸen wie mit dem Universe Z23 Ultra. Bei Standardfotos stimmen SchÃ¤rfe und Helligkeit, auch wenn Bilder stÃ¤rker nachgeschÃ¤rft werden als beim Top-Modell. Die Farben weichen teils von der RealitÃ¤t ab, das Z23 Ultra zeigt natÃ¼rlichere Farben an. Die Akkulaufzeit ist gut - nur das Aufladen geht zu langsam. Unterm Strich bekommen Sie aber sehr viel geboten fÃ¼r Ihr Geld.*",
      "math_blocks": [],
      "images": [],
      "options": [
        "(Universe Z23 Ultra, Aufladen, positiv)",
        "(Universe X42, Akkulaufzeit, positiv)",
        "(Universe X42, Bildhelligkeit, negativ)",
        "(Universe X42, Preis-LeistungsverhÃ¤ltnis, positiv)"
      ],
      "correct_options": ["B", "D"],
      "given_answer": "Korrekte Tupel sind (Universe X42, Akkulaufzeit, positiv) und (Universe X42, Preis-LeistungsverhÃ¤ltnis, positiv).",
      "verified": true,
      "notes": "Transkribiert und korrigiert von PDF Seite 10."
    },
    {
        "id": "Q20",
        "source": "NLP_Cluster_aufgaben.pdf_11",
        "type": "mc_check",
        "topic": "Skip-Gramme",
        "question_text": "Welche 1-Skip-4-Gramme kommen in dem folgenden Text - ohne BerÃ¼cksichtigung von Satzzeichen - vor:\n\n*Als Gregor Samsa eines Morgens aus unruhigen TrÃ¤umen erwachte, fand er sich in seinem Bett zu einem ungeheueren Ungeziefer verwandelt.*",
        "math_blocks": [],
        "images": [],
        "options": [
            "fand in seinem Bett",
            "Bett zu ungeheueren Ungeziefer",
            "aus unruhigen TrÃ¤umen erwachte",
            "zu ungeheueren Ungeziefer verwandelt",
            "Gregor Samsa Morgens unruhigen",
            "fand er sich in seinem Bett"
        ],
        "correct_options": ["C", "D"],
        "given_answer": "Korrekte 1-Skip-4-Gramme sind 'aus unruhigen TrÃ¤umen erwachte' und 'zu ungeheueren Ungeziefer verwandelt'.",
        "verified": true,
        "notes": "Transkribiert und korrigiert von PDF Seite 11."
    },
    {
        "id": "Q21",
        "source": "NLP_Cluster_aufgaben.pdf_12",
        "type": "mc_radio",
        "topic": "Sprachmodelle",
        "question_text": "Eine Forscherin/ein Forscher hat herausgefunden, dass sich die Autorenschaft eines Textes durch Muster in kurzen Sequenzen von Verben in Texten feststellen lÃ¤sst. Diese Muster bestehen in Form von Sequenzen von Verben der MindestlÃ¤nge 2, allerdings muss dabei manchmal ein Verb Ã¼bersprungen werden und die GesamtlÃ¤nge der Sequenz darf 3 nicht Ã¼berschreiten. Die Verbsequenzen kÃ¶nnen sich Ã¼ber Satzgrenzen hinaus erstrecken, allerdings nicht Ã¼ber unterschiedliche Paragraphen. Welches der folgenden Sprachmodelle ist geeignet, um diese Muster zu erkennen?",
        "math_blocks": [],
        "images": [],
        "options": [
            "Bag-of-3-Skip-1-Grams-of-Verb-Token innerhalb des gleichen Paragraphen",
            "Die Vereinigung von 1-Skip-2-Grams- und 1-Skip-3-Grams-of-Verb-Token innerhalb des gleichen Paragraphen",
            "Bag-of-1-Skip-3-Grams-of-Verb-Token innerhalb des gesamten Textes",
            "Die Vereinigung von 1-Skip-2-Grams- und 2-Skip-3-Grams-of-Verb-Token innerhalb des gleichen Paragraphen",
            "Bag-of-1-Skip-3-Grams-of-Verb-Token innerhalb des gleichen Paragraphen"
        ],
        "correct_options": ["B"],
        "given_answer": "Die korrekte Antwort ist 'Die Vereinigung von 1-Skip-2-Grams- und 1-Skip-3-Grams-of-Verb-Token innerhalb des gleichen Paragraphen', da dies Sequenzen der LÃ¤nge 2 und 3 mit jeweils bis zu einem Skip abbildet.",
        "verified": true,
        "notes": "Transkribiert und korrigiert von PDF Seite 12."
    },
    {
        "id": "Q22",
        "source": "NLP_Cluster_aufgaben.pdf_13",
        "type": "mc_radio",
        "topic": "Sprachmodelle",
        "question_text": "In stark flektierenden Sprachen, wie beispielsweise Deutsch, haben WÃ¶rter in der Regel viele unterschiedliche Wortformen. Sie wollen nun Word Embeddings trainieren, dessen ReprÃ¤sentationen invarianter gegenÃ¼ber geringfÃ¼gigen Ã„nderungen von Wortformen des selben Wortes sind, als es das word2vec Modell ist. Dabei wollen Sie aber vermeiden, dass das Modell alle Wortformen eines Wortes gleich reprÃ¤sentiert. Mit welchem der folgenden AnsÃ¤tze kann Ihr Vorhaben gelingen?",
        "math_blocks": [],
        "images": [],
        "options": [
            "WÃ¶rter werden durch Buchstaben n-Gramme reprÃ¤sentiert und Word Embeddings als die Summe der n-Gram Embeddings berechnet.",
            "WÃ¶rter werden durch Buchstaben n-Gramme reprÃ¤sentiert und Word Embeddings als die Konkatenierung der n-Gram Embeddings berechnet.",
            "Sie extrahieren syntaktische Informationen und konkatenieren entsprechende Syntax-Merkmal-Embeddings mit dem Embedding der Wortform.",
            "Sie verwenden die Lemmata der WÃ¶rter, anstelle der Wortformen."
        ],
        "correct_options": ["A"],
        "given_answer": "Die korrekte Antwort ist A. Durch die ReprÃ¤sentation Ã¼ber Buchstaben-n-Gramme und die Summierung der Embeddings (wie bei FastText) werden Ã¤hnliche Wortformen Ã¤hnliche Vektoren haben, ohne identisch zu sein.",
        "verified": true,
        "notes": "Transkribiert und korrigiert von PDF Seite 13."
    }
  ]
}