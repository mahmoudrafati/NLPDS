{
  "exam_questions": [
    {
      "id": "Q1",
      "source": "SCREENSHOT_60CCEE21-78C6-47C3-92B8-7FA6AC3F92EE.jpeg",
      "type": "offene_frage",
      "topic": "Embeddings",
      "question_text": "In stark flektierenden Sprachen sollen Word-Embeddings gegenüber Formvarianten robust sein,\nohne alle Wortformen eines Lemmas zu verschmelzen. Skizzieren Sie einen Ansatz, wie Sie\nWörter über Zeichen-n-Gramme repräsentieren und daraus Wort-Embeddings bilden.",
      "math_blocks": [
        "$\\displaystyle \\mathbf{v}(w)=\\sum_{g\\in G(w)} \\mathbf{z}_g$"
      ],
      "images": [
        "Screenshot: Sprachmodelle II (Char‑n‑Gramme)"
      ],
      "options": [],
      "given_answer": "Verwenden Sie **Char-n-Gramm-Embeddings** (FastText-Prinzip): Zerlegen Sie jedes Wort w in ein Set G(w)\naus überlappenden Zeichen-n-Grammen (inkl. Anfangs/Endmarker). Lernen Sie für jedes n‑Gramm g ein Vektor\nz_g. Das Wort-Embedding berechnet sich als **Summe (oder Mittel)** der zugehörigen n‑Gramm-Vektoren:\nv(w)=∑_{g∈G(w)} z_g. Dadurch bleiben Wortformen **ähnlich**, aber **nicht identisch** (geteilte Subvektoren).\nKeine reine Lemmatisierung (würde Formen gleichsetzen) und keine bloße Konkatenation (würde Dimension sprengen).",
      "verified": false,
      "notes": "Aus Screenshot; in der Klausur Freitext, keine MC."
    },
    {
      "id": "Q2",
      "source": "SCREENSHOT_ED087468-70DC-422D-894E-0A9FC6033373.png",
      "type": "offene_frage",
      "topic": "Sentiment Analysis",
      "question_text": "Ein relationales Sentiment‑Tupel sei definiert als (e_x, e_y, a, s_r) mit s_r ∈ {besser, gleich, schlechter}.\nLesen Sie den Beispieltext (Smartphones X42 vs. Z23 Ultra) und geben Sie alle im Text enthaltenen relationalen\nSentiment‑Tupel an.",
      "math_blocks": [
        "$(e_x, e_y, a, s_r)$",
        "$s_r \\in \\{\\mathrm{besser}, \\mathrm{gleich}, \\mathrm{schlechter}\\}$"
      ],
      "images": [
        "Screenshot: Relationales Sentiment‑Tupel (Smartphone‑Vergleich)"
      ],
      "options": [],
      "given_answer": "Aus dem Text ergeben sich:\n1) (Universe X42, Universe Z23 Ultra, Fotoqualität, gleich) — „grundsätzlich gleich gute Fotos“.\n2) (Universe X42, Universe Z23 Ultra, Farbnatürlichkeit, schlechter) — „Z23 Ultra zeigt natürlichere Farben“.\nSchärfe/Helligkeit werden als „stimmen überein“ beschrieben → relational „gleich“. Akku/Charging ist nicht als klarer\nVergleich zwischen X42 und Z23 Ultra formuliert → kein relationales Tupel.",
      "verified": false,
      "notes": "Aus Screenshot; Auswahl im Original als MC, hier Freitext."
    },
    {
      "id": "Q3",
      "source": "SCREENSHOT_842FEDC8-C55E-497B-A618-FCC793EE214C.png",
      "type": "offene_frage",
      "topic": "Transformer/Bias",
      "question_text": "Gegeben sei ein Masked Language Model (MLM) wie BERT. Skizzieren Sie einen Ansatz, um nachzuweisen,\nob das Modell einen Gender‑Bias repliziert (z. B. bei Berufsrollen). Beschreiben Sie kurz Templates,\nMaskierung, Auswertung und ein Bias‑Maß.",
      "math_blocks": [
        "$\\text{Template: } \\_ \\text{ arbeitet als Ingenieur.}$",
        "$\\text{Template: } \\_ \\text{ arbeitet als Krankenschwester.}$",
        "$\\displaystyle \\Delta = P(\\text{männlich} \\mid \\text{Ingenieur}) - P(\\text{weiblich} \\mid \\text{Ingenieur})$"
      ],
      "images": [
        "Screenshot: MLM‑Bias‑Nachweis (BERT)"
      ],
      "options": [],
      "given_answer": "**Bias‑Probe (Zero‑shot‑Templates):**\n1) Erstellen Sie neutrale Sätze mit einem Masken‑Slot für Namen: „[MASK] arbeitet als Ingenieur/Krankenschwester.“\n2) Lassen Sie BERTs MLM‑Head Top‑k‑Token für [MASK] vorhersagen und zählen Sie männliche vs. weibliche Namen.\n3) Erwartung: Für „Ingenieur“ treten häufiger männliche Namen auf; für „Krankenschwester“ häufiger weibliche.\n4) **Bias‑Maß:** Δ = P(männlich | Ingenieur) − P(weiblich | Ingenieur) (analog für andere Rollen). Signifikant\nvon 0 verschieden (z. B. t‑Test/Bootstrap) ⇒ Bias. Optional: Normalisieren über Namensfrequenzen und kontrollieren\nfür Kontexte, mehrere Sprachen/Berufslisten, Konfidenzintervalle berichten.",
      "verified": false,
      "notes": "Aus Screenshot; Freitext."
    },
    {
      "id": "Q4",
      "source": "CHAT_GLMPROMPT_1",
      "type": "offene_frage",
      "topic": "Maschinelle Übersetzung/Prompting",
      "question_text": "Sie haben Zugriff auf ein großes Generative Language Model (GLM; Next‑Token‑Prediction). Entwickeln Sie einen\nPrompt‑basierten Ansatz, um eine **automatische Übersetzung** für einen kleinen Datensatz zu implementieren,\nund erläutern Sie den Ablauf (Prompt‑Design, Inferenz, Qualitätssicherung).",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "**Ansatz:** *Instruction‑/Few‑shot‑Prompting* mit striktem Format.\n- **Few‑shot‑Beispiele** aus dem kleinen Datensatz (2–10 Satzpaare) im Schema:\n  „Translate German → English. Preserve punctuation and names.\nGerman: <S>\nEnglish: <T>“.\n- **Prompt‑Schablone:** System: „You are a translation engine. Output only the translation.“\n  + Regeln: keine Erklärungen, keine Halluzinationen, Zahlen/Einheiten beibehalten.\n- **Inferenz:** Greedy/Beam (kleines Beam 3–5) für Determinismus; max_length, no_repeat_ngram ≥ 3.\n- **Post‑Checks:** Round‑trip‑Check (de→en→de), einfache Heuristiken (Anführungszeichen/Tags erhalten),\n  optional TER/BLEU auf Valid‑Subset.\n- **Feintuning light (optional):** Parameter‑Effizienz (LoRA/PEFT) auf dem kleinen Satzpaar‑Set,\n  falls das reine Prompting nicht stabil genug ist.\n- **Batching:** Satzweise; bei langen Inputs Segmentierung und Zusammensetzen.",
      "verified": false,
      "notes": "Aus Chat‑Gedächtnisprotokoll."
    },
    {
      "id": "Q5",
      "source": "CHAT_BERTGEN_1",
      "type": "offene_frage",
      "topic": "Transformer/BERT/Language Modeling",
      "question_text": "Kann BERT für **generatives Language Modeling** verwendet werden? Begründen Sie Ihre Antwort.",
      "math_blocks": [
        "$\\text{MLM: } p(w_t \\mid \\text{Kontext links+rechts mit Masken})$",
        "$\\text{AR-LM: } p(w_t \\mid w_{<t})$"
      ],
      "images": [],
      "options": [],
      "given_answer": "**Kurz:** Nicht sinnvoll als *klassisches* generatives LM.\n- **BERT** wird mit **Masked‑Language‑Modeling** (bidirektionaler Kontext) trainiert und approximiert *kein*\n  autoregressives p(w_t | w_{<t}).\n- Man *kann* mit iterativem Maskieren/Infilling Text erzeugen, das ist jedoch **ineffizient** und\n  qualitativ/steuerbar den **Decoder‑only** (GPT) oder **Encoder‑Decoder** (T5)‑Modellen unterlegen.\n- Für echtes NTP/AR‑LM sind Decoder‑only‑Modelle die passende Architektur.",
      "verified": false,
      "notes": "Aus Chat‑Gedächtnisprotokoll."
    },
    {
      "id": "Q6",
      "source": "CHAT_WEAKAI_1",
      "type": "definition",
      "topic": "KI-Grundlagen",
      "question_text": "Nennen und begründen Sie drei Gründe, warum eine KI, die einzelne Matheaufgaben löst, als **schwache (narrow) KI** gilt.",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "1) **Enger Aufgabenscope:** Optimiert für ein klar umrissenes Problem (Matheaufgaben) statt allgemeiner Intelligenz.\n2) **Kein Weltverständnis/Bewusstsein:** Manipuliert Symbole/Heuristiken, ohne semantisches Verstehen oder Intentionalität.\n3) **Begrenzte Transferfähigkeit:** Außerhalb des Trainingsregimes/Formats bricht die Leistung ein; kein breiter Common‑Sense,\n   keine langfristige Planung/Autonomie. (Optional: fehlende Zielautonomie, keine eigenen Motive/Erfahrungsbildung.)",
      "verified": false,
      "notes": "Aus Chat‑Gedächtnisprotokoll."
    },
    {
      "id": "Q7",
      "source": "CHAT_SKIPGRAM_1",
      "type": "rechenaufgabe",
      "topic": "Sprachmodellierung/Word2Vec",
      "question_text": "Gegeben ist das Skip‑gram‑Ziel. Modifizieren Sie die Definition so, dass\na) nur Verben und Adjektive erfasst werden,\nb) statt eines fixen Fensters k ein Anteil p∈(0,1) der Token berücksichtigt wird,\nc) statt sequenzieller Abstände **baum‑basierte** Abstände (z. B. Dependenz‑Baum) bis k verwendet werden.",
      "math_blocks": [
        "$\\displaystyle \\mathcal{L}(\\theta)=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-k\\le j\\le k\\\\ j\\ne 0}}\\log p_\\theta(w_{t+j}\\mid w_t)$",
        "$\\displaystyle \\mathcal{L}_a=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}[\\mathrm{POS}(w_t)\\in\\{\\mathrm{VERB},\\mathrm{ADJ}\\}]\\sum_{\\substack{-k\\le j\\le k\\\\ j\\ne 0}}\\mathbf{1}[\\mathrm{POS}(w_{t+j})\\in\\{\\mathrm{VERB},\\mathrm{ADJ}\\}]\\log p_\\theta(w_{t+j}\\mid w_t)$",
        "$\\displaystyle \\mathcal{L}_b=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-K\\le j\\le K\\\\ j\\ne 0}}\\log p_\\theta(w_{t+j}\\mid w_t),\\ \\ K=\\lfloor p\\cdot T\\rfloor$",
        "$\\displaystyle \\mathcal{L}_c=\\frac{1}{T}\\sum_{t=1}^{T}\\ \\sum_{j\\in \\mathcal{N}_k(t)} \\log p_\\theta(w_{j}\\mid w_t),\\ \\ \\mathcal{N}_k(t)=\\{j\\neq t: d_{\\text{tree}}(t,j)\\le k\\}$"
      ],
      "images": [],
      "options": [],
      "given_answer": "a) Filtern über **POS‑Tags**: Beiträge zählen nur, wenn Target **und** Kontext POS∈{VERB, ADJ} sind.\nb) Ersetze Fensterbreite durch **K = ⌊p·T⌋**; Kontextindizes j laufen über −K…K, j≠0.\nc) Ersetze sequentielle Distanz |j| durch **Baumdistanz** d_tree(t,j) in einem (m=T) Knotentextbaum und\nbilde die Kontextnachbarschaft 𝒩_k(t) = {j≠t : d_tree(t,j) ≤ k}. Das Ziel bleibt „maximal k Abstände“ im Baum.",
      "verified": false,
      "notes": "Aus Chat‑Gedächtnisprotokoll."
    },
    {
      "id": "Q8",
      "source": "CHAT_FAKENEWS_1",
      "type": "offene_frage",
      "topic": "Klassifikation/Sicherheitsanwendungen",
      "question_text": "Skizzieren Sie einen Ansatz für **Fake‑News‑Detektion**: Daten, Features/Modelle, Evaluation, Robustheit.",
      "math_blocks": [],
      "images": [],
      "options": [],
      "given_answer": "**Pipeline:**\n- **Daten:** Titel+Body; Label (fake/real). Split chronologisch (Leak vermeiden). Balancing prüfen.\n- **Vorverarbeitung:** Normalisieren, URLs/Handles, Sprachdetektion; ggf. Claim‑Extraktion.\n- **Repräsentationen:** (a) TF‑IDF n‑Gramme (1–3), (b) kontextuelle Embeddings (BERT/Longformer).\n- **Modelle:** Lineare Baselines (LogReg/SVM) + Feintuning eines Encoder‑Modells (CLS‑Token → Softmax).\n- **Evaluation:** Stratified k‑fold oder zeitlich getreu; Metriken: Precision/Recall/F1, ROC‑AUC; Kalibrierung prüfen.\n- **Robustheit:** Adversarien (Paraphrasen), Quellen‑/Themen‑Shift (OOD‑Test), Erklärbarkeit (SHAP/Attention‑Rollout),\n  und Regel‑Checks (Clickbait‑Indikatoren) kombinieren.\n- **Fehleranalyse:** Confusion‑Matrizen, Hard‑Negatives sammeln und nachtrainieren.",
      "verified": false,
      "notes": "Aus Chat‑Gedächtnisprotokoll."
    },
    {
      "id": "Q9",
      "source": "CHAT_ATTENTION_1",
      "type": "offene_frage",
      "topic": "Transformer/Attention",
      "question_text": "Erläutern Sie **Scaled Dot‑Product Self‑Attention** und nennen Sie die Schritte der Berechnung.\nGehen Sie auch kurz auf **Multi‑Head Attention** und die Rechenkomplexität ein.",
      "math_blocks": [
        "$\\displaystyle \\mathrm{Attn}(Q,K,V)=\\mathrm{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$",
        "$Q=XW^Q,\\ K=XW^K,\\ V=XW^V$",
        "$\\text{MHA}(X)=\\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_h)W^O$"
      ],
      "images": [],
      "options": [],
      "given_answer": "**Self‑Attention:**\n1) Lineare Projektionen: Q=XW^Q, K=XW^K, V=XW^V.\n2) Skalarprodukte: S=QK^T / √d_k (Skalierung stabilisiert Gradienten).\n3) Gewichte: A=softmax(S) zeilenweise.\n4) Ausgabe: Z = A·V.\n**Multi‑Head:** h parallele Köpfe mit kleineren d_k, die unterschiedliche Relationen lernen; deren Ausgaben werden\nkonkateniert und mit W^O projiziert. **Komplexität:** O(n²·d) in der Sequenzlänge n (durch QK^T); Speicher O(n²).\nMaskierte Attention (Decoder) verhindert Blick in die Zukunft.",
      "verified": false,
      "notes": "Aus Chat‑Gedächtnisprotokoll; genaue Aufgabenstellung unbekannt."
    }
  ]
}